{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Property with Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"dataset/processed/241212\"\n",
    "data = []\n",
    "for filename in os.listdir(dir_path):\n",
    "    if filename.endswith(\".json\"):\n",
    "        with open(os.path.join(dir_path, filename), 'r') as f:\n",
    "            data.extend(json.load(f))\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [dict(\n",
    "    noun_phrase = line[0],\n",
    "    sentence = line[1],\n",
    "    source = line[2]\n",
    ") for line in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openai import OpenAI\n",
    "# import os\n",
    "# import json\n",
    "\n",
    "# template = \"\"\"Given the sentence and noun phrase, explain the property of the noun phrase.\n",
    "\n",
    "# ---\n",
    "# Sentence: You know, we are all kind of like a deer in the headlights.\n",
    "# Noun phrase: a deer in the headlights\n",
    "# Property: scared\n",
    "# ---\n",
    "# Sentence: We saw like, it looked, I mean, it looked almost like a big bus and I don't really know what that was for but it was driving around as well.\n",
    "# Noun phrase: a big bus\n",
    "# Property: heavy\n",
    "# ---\n",
    "# Sentence: Image suddenly became as important as the music for artists.\n",
    "# Noun phrase: music for artists\n",
    "# Property: important\n",
    "# ---\n",
    "# Sentence: {sentence}.\n",
    "# Noun phrase: {noun_phrase}\n",
    "# Property: \"\n",
    "# \"\"\"\n",
    "\n",
    "# prompts = [dict(\n",
    "#     custom_id = f\"request-{i+1}\",\n",
    "#     method = \"POST\",\n",
    "#     url = \"/v1/chat/completions\",\n",
    "#     body = dict(\n",
    "#         model = \"gpt-4o-mini\",\n",
    "#         messages = [\n",
    "#             {\"role\": \"user\", \"content\": template.format(**line)}\n",
    "#         ],\n",
    "#         max_tokens = 100,\n",
    "#         n=10,\n",
    "#     )\n",
    "# ) for i, line in enumerate(data)]\n",
    "\n",
    "# client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "# save_path = \"dataset/processed/241212/property_extraction__gpt4o_miny__chunk_{}.jsonl\"\n",
    "\n",
    "# for i in range(0, len(prompts), 50000):\n",
    "#     save_path_chunk = save_path.format(i)\n",
    "#     with open(save_path_chunk, 'w') as outfile:\n",
    "#         for entry in prompts[i:i+50000]:\n",
    "#             json.dump(entry, outfile)\n",
    "#             outfile.write('\\n')\n",
    "\n",
    "#     batch_input_file = client.files.create(\n",
    "#         file=open(save_path_chunk, \"rb\"),\n",
    "#         purpose=\"batch\"\n",
    "#     )\n",
    "\n",
    "#     batch_input_file_id = batch_input_file.id\n",
    "\n",
    "#     client.batches.create(\n",
    "#         input_file_id=batch_input_file_id,\n",
    "#         endpoint=\"/v1/chat/completions\",\n",
    "#         completion_window=\"24h\",\n",
    "#         metadata={\n",
    "#         \"description\": \"extract property of noun phrase from sentence\"\n",
    "#         }\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "emphasis_mark = [\"\\*\", \"\\*\\*\", \"\\\"\"]\n",
    "colon_mark = \":\"\n",
    "\n",
    "class OpenAI_Output:\n",
    "    def __init__(self, dir_path):\n",
    "        self.start_name = \"batch\"\n",
    "        self.file_paths = []\n",
    "        for filename in os.listdir(dir_path):\n",
    "            if filename.startswith(self.start_name):\n",
    "                self.file_paths.append(os.path.join(dir_path, filename))\n",
    "        self.data = dict()\n",
    "        for file_path in self.file_paths:\n",
    "            with jsonlines.open(file_path) as reader:\n",
    "                for obj in reader:\n",
    "                    key = int(obj['custom_id'].split(\"-\")[1])\n",
    "                    val = [response['message']['content'] for response in obj['response']['body']['choices']]\n",
    "                    self.data[key] = val\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        if key in self.data:\n",
    "            return self.data[key]\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    def extract_properties(self, key):\n",
    "        outputs = self[key]\n",
    "        properties = []\n",
    "        for output in outputs:\n",
    "            sentences = output.split(\"\\n\")\n",
    "            for sentence in sentences:\n",
    "                # property is described after colon mark\n",
    "                if colon_mark in sentence:\n",
    "                    property = sentence.split(colon_mark)[-1]\n",
    "                    properties.append(property.lower())\n",
    "                # property is enclosed with emphasis mark\n",
    "                for mark in emphasis_mark:\n",
    "                    property = re.findall(f\"{mark}([^\\s{mark}]+?){mark}\", sentence)\n",
    "                    if property:\n",
    "                        properties.extend(property)\n",
    "        \n",
    "        # extract english characters with dash (-)\n",
    "        properties = [property.strip().lower() for property in properties if len(property.strip()) > 5]\n",
    "        properties = [property for property in properties if not any([mark for mark in emphasis_mark if mark.replace(\"\\\\\", \"\") in property])]\n",
    "        properties = [re.findall(r'[a-zA-Z-]+', property) for property in properties]\n",
    "        properties = [property[0] for property in properties if property]\n",
    "        properties = [property for property in properties if 'propert' not in property]\n",
    "        properties = list(set(properties))\n",
    "        return properties\n",
    "    \n",
    "dir_path = \"dataset/processed/241212_result\"\n",
    "openai_output = OpenAI_Output(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [dict(line, index=i+1, properties=openai_output.extract_properties(i+1)) for i, line in enumerate(tqdm(data))]\n",
    "\n",
    "# sentence 중복 제거\n",
    "sentences = []\n",
    "new_data = []\n",
    "for line in tqdm(data):\n",
    "    if line['noun_phrase']+line['sentence'] not in sentences:\n",
    "        sentences.append(line['noun_phrase']+line['sentence'])\n",
    "        new_data.append(line)\n",
    "\n",
    "# re-indexing data\n",
    "new_data = [dict(line, index=i+1) for i, line in enumerate(new_data)]\n",
    "new_data[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset/processed/241212_property_extraction.jsonl\", 'w') as outfile:\n",
    "    for line in new_data:\n",
    "        json.dump(line, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make commonsense statement (Out-dated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import transformers\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = \"dataset/processed/241212_property_extraction.jsonl\"\n",
    "\n",
    "with jsonlines.open(fpath) as f:\n",
    "    data = [line for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question formatter\n",
    "\n",
    "model_name = 'domenicrosati/question_converter-3b'\n",
    "\n",
    "config = transformers.AutoConfig.from_pretrained(model_name)\n",
    "format_tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "format_model = transformers.AutoModelForSeq2SeqLM.from_pretrained(model_name, config=config)\n",
    "format_model.to(device)\n",
    "print('decoder start token id:', format_model.config.decoder_start_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def convert_question(inputs, batch_size=128):\n",
    "    sentences = []\n",
    "    chunk = []\n",
    "    \n",
    "    for noun_phrase, properties in inputs:\n",
    "        for p in properties:\n",
    "            sentences.append(f\"How is {noun_phrase} generally? </s> {p}\")\n",
    "        chunk.append(len(properties))\n",
    "\n",
    "    statements = []\n",
    "    \n",
    "    for batch in tqdm(range(0, len(sentences), batch_size)):\n",
    "        s = sentences[batch:batch+batch_size]\n",
    "        input_ids = format_tokenizer.batch_encode_plus(s, return_tensors='pt', padding=True, truncation=True).input_ids.to(device)\n",
    "        output = format_model.generate(input_ids=input_ids, max_length=256)\n",
    "        d = format_tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "        statements.extend(d)\n",
    "        \n",
    "    return [statements[i:j] for i, j in zip([0] + list(accumulate(chunk)), list(accumulate(chunk)))]\n",
    "\n",
    "# convert_question(\"a big bus\", \"scary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "for line in data:\n",
    "    inputs.append((line['noun_phrase'], line['properties']))\n",
    "    \n",
    "outputs = convert_question(inputs)\n",
    "\n",
    "for output, line in zip(outputs, data):\n",
    "    line['statements'] = output\n",
    "    line['statements'] = [s if len(p) > 2 else None for s, p in zip(line['statements'], line['properties'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset/processed/241212_statements.jsonl\", 'w') as outfile:\n",
    "    for line in data:\n",
    "        json.dump(line, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter statement by VERA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import transformers\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = \"dataset/processed/241212_property_extraction.jsonl\"\n",
    "\n",
    "with jsonlines.open(fpath) as f:\n",
    "    data = [line for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vera_tokenizer = transformers.AutoTokenizer.from_pretrained('liujch1998/vera')\n",
    "vera_model = transformers.T5EncoderModel.from_pretrained('liujch1998/vera')\n",
    "\n",
    "vera_model.to(device)\n",
    "\n",
    "vera_model.D = vera_model.shared.embedding_dim\n",
    "linear = torch.nn.Linear(vera_model.D, 1, dtype=vera_model.dtype)\n",
    "linear.weight = torch.nn.Parameter(vera_model.shared.weight[32099, :].unsqueeze(0))\n",
    "linear.bias = torch.nn.Parameter(vera_model.shared.weight[32098, 0].unsqueeze(0))\n",
    "vera_model.eval()\n",
    "t = vera_model.shared.weight[32097, 0].item() # temperature for calibration\n",
    "\n",
    "def get_score(statements):\n",
    "    scores = []\n",
    "    for statement in statements:\n",
    "        if isinstance(statement, str):\n",
    "            input_ids = vera_tokenizer.batch_encode_plus([statement], return_tensors='pt', padding='longest', truncation='longest_first', max_length=128).input_ids.to(vera_model.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output = vera_model(input_ids)\n",
    "                last_hidden_state = output.last_hidden_state\n",
    "                hidden = last_hidden_state[:, -1, :]\n",
    "                logit = linear(hidden).squeeze(-1)\n",
    "                logit_calibrated = logit / t\n",
    "                score_calibrated = logit_calibrated.sigmoid()\n",
    "                scores.append(score_calibrated[0])\n",
    "        else:\n",
    "            scores.append(0.0)\n",
    "    \n",
    "    return [float(score) for score in scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [dict(line, scores=get_score([f\"The {line['noun_phrase']} is generally {property} .\" if len(property) > 2 else None for property in line['properties']])) for line in tqdm(data)]\n",
    "\n",
    "with open(\"dataset/processed/241212_vera.jsonl\", 'w') as outfile:\n",
    "    for line in data:\n",
    "        json.dump(line, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = \"data/pipeline/241212_vera.jsonl\"\n",
    "\n",
    "df = pd.read_json(fpath, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processed_properties'] = df.apply(lambda x: [p for p, s in zip(x['properties'], x['scores']) if s > 0.7], axis=1)\n",
    "df['processed_properties'].apply(len).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['index', 'properties', 'scores'], inplace=True)\n",
    "df = df.explode('processed_properties').rename(columns={'processed_properties': 'property'})\n",
    "df.dropna(subset=['property'], inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"data/pipeline/241212_vera_processed.jsonl\", orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (...) Make Canceled property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jsonlines\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "df = pd.read_csv(\"data/pipeline/emergent_seeds_flag.csv\")\n",
    "df = df[df.flag]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import sys\n",
    "from data.MAPSKB.models import MAPSKB\n",
    "\n",
    "fpath = \"data/MAPSKB/MAPS-KB.csv\"\n",
    "mapskb = MAPSKB(fpath)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp(\"veiled\")[0].lemma_\n",
    "\n",
    "def get_property(concept):\n",
    "    first_properties = mapskb.SI(concept)\n",
    "    if len(first_properties) == 0:\n",
    "        second_properties = mapskb.SI(nlp(concept)[0].lemma_)\n",
    "        return second_properties\n",
    "    else:\n",
    "        return first_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openai import OpenAI\n",
    "# import os\n",
    "# import json\n",
    "\n",
    "# template = \"\"\"What is the property of the concept?\n",
    "\n",
    "# ---\n",
    "# Concept: car\n",
    "# Property: fast\n",
    "# ---\n",
    "# Concept: apple\n",
    "# Property: round\n",
    "# ---\n",
    "# Concept: needle\n",
    "# Property: sharp\n",
    "# ---\n",
    "# Concept: {concept}\n",
    "# Property: \"\n",
    "# \"\"\"\n",
    "\n",
    "# prompts = [dict(\n",
    "#     custom_id = f\"{concept}\",\n",
    "#     method = \"POST\",\n",
    "#     url = \"/v1/chat/completions\",\n",
    "#     body = dict(\n",
    "#         model = \"gpt-4o-mini\",\n",
    "#         messages = [\n",
    "#             {\"role\": \"user\", \"content\": template.format(concept=concept)}\n",
    "#         ],\n",
    "#         max_tokens = 100,\n",
    "#         n=10,\n",
    "#     )\n",
    "# ) for concept in concepts]\n",
    "\n",
    "# client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "# save_path = \"data/pipeline/241227/property_extraction__gpt4o_miny__chunk_{}.jsonl\"\n",
    "\n",
    "# for i in range(0, len(prompts), 50000):\n",
    "#     save_path_chunk = save_path.format(i)\n",
    "#     with open(save_path_chunk, 'w') as outfile:\n",
    "#         for entry in prompts[i:i+50000]:\n",
    "#             json.dump(entry, outfile)\n",
    "#             outfile.write('\\n')\n",
    "\n",
    "#     batch_input_file = client.files.create(\n",
    "#         file=open(save_path_chunk, \"rb\"),\n",
    "#         purpose=\"batch\"\n",
    "#     )\n",
    "\n",
    "#     batch_input_file_id = batch_input_file.id\n",
    "\n",
    "#     client.batches.create(\n",
    "#         input_file_id=batch_input_file_id,\n",
    "#         endpoint=\"/v1/chat/completions\",\n",
    "#         completion_window=\"24h\",\n",
    "#         metadata={\n",
    "#         \"description\": \"extract property of concept\"\n",
    "#         }\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emphasis_mark = [\"\\*\", \"\\*\\*\", \"\\\"\"]\n",
    "# colon_mark = \":\"\n",
    "\n",
    "# class OpenAI_Output:\n",
    "#     def __init__(self, dir_path):\n",
    "#         self.start_name = \"batch\"\n",
    "#         self.file_paths = []\n",
    "#         for filename in os.listdir(dir_path):\n",
    "#             if filename.startswith(self.start_name):\n",
    "#                 self.file_paths.append(os.path.join(dir_path, filename))\n",
    "#         self.data = dict()\n",
    "#         for file_path in self.file_paths:\n",
    "#             with jsonlines.open(file_path) as reader:\n",
    "#                 for obj in reader:\n",
    "#                     key = obj['custom_id']\n",
    "#                     val = [response['message']['content'] for response in obj['response']['body']['choices']]\n",
    "#                     self.data[key] = val\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "        \n",
    "#     def __getitem__(self, key):\n",
    "#         if key in self.data:\n",
    "#             return self.data[key]\n",
    "#         else:\n",
    "#             return []\n",
    "    \n",
    "#     def extract_properties(self, key):\n",
    "#         outputs = self[key]\n",
    "#         properties = []\n",
    "#         for output in outputs:\n",
    "#             sentences = output.split(\"\\n\")\n",
    "#             for sentence in sentences:\n",
    "#                 # property is described after colon mark\n",
    "#                 if colon_mark in sentence:\n",
    "#                     property = sentence.split(colon_mark)[-1]\n",
    "#                     properties.append(property.lower())\n",
    "#                 # property is enclosed with emphasis mark\n",
    "#                 for mark in emphasis_mark:\n",
    "#                     property = re.findall(f\"{mark}([^\\s{mark}]+?){mark}\", sentence)\n",
    "#                     if property:\n",
    "#                         properties.extend(property)\n",
    "        \n",
    "#         # extract english characters with dash (-)\n",
    "#         properties = [property.strip().lower() for property in properties if len(property.strip()) > 5]\n",
    "#         properties = [property for property in properties if not any([mark for mark in emphasis_mark if mark.replace(\"\\\\\", \"\") in property])]\n",
    "#         properties = [re.findall(r'[a-zA-Z-]+', property) for property in properties]\n",
    "#         properties = [property[0] for property in properties if property]\n",
    "#         properties = [property for property in properties if 'propert' not in property]\n",
    "#         properties = list(set(properties))\n",
    "#         return properties\n",
    "    \n",
    "# dir_path = \"data/pipeline/241227_result\"\n",
    "# openai_output = OpenAI_Output(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['root_properties'] = df['root'].progress_apply(get_property)\n",
    "df['modifier_properties'] = df['modifier'].progress_apply(get_property)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame to store the expanded rows\n",
    "expanded_rows = []\n",
    "\n",
    "# Iterate over each row in the original DataFrame\n",
    "for _, row in df.iterrows():\n",
    "    # Create a new row for the root properties\n",
    "    for root_property in row['root_properties']:\n",
    "        new_row = row.copy()\n",
    "        new_row['property'] = root_property\n",
    "        new_row['root_rel'] = None\n",
    "        new_row['modifier_rel'] = None\n",
    "        new_row['noun_phrase_rel'] = None\n",
    "        new_row['individual_max'] = None\n",
    "        new_row['ccpt_score'] = None\n",
    "        expanded_rows.append(new_row)\n",
    "\n",
    "    # Create a new row for the modifier properties\n",
    "    for modifier_property in row['modifier_properties']:\n",
    "        new_row = row.copy()\n",
    "        new_row['property'] = modifier_property\n",
    "        new_row['root_rel'] = None\n",
    "        new_row['modifier_rel'] = None\n",
    "        new_row['noun_phrase_rel'] = None\n",
    "        new_row['individual_max'] = None\n",
    "        new_row['ccpt_score'] = None\n",
    "        expanded_rows.append(new_row)\n",
    "\n",
    "# Create a new DataFrame from the expanded rows\n",
    "expanded_df = pd.DataFrame(expanded_rows)\n",
    "\n",
    "# Display the new DataFrame\n",
    "expanded_df.reset_index(drop=True, inplace=True)\n",
    "expanded_df.drop(columns=['root_properties', 'modifier_properties'], inplace=True)\n",
    "expanded_df.to_csv(\"data/pipeline/241227_expanded.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = \"dataset/processed/passed.jsonl\"\n",
    "\n",
    "with jsonlines.open(fpath) as f:\n",
    "    data = [line for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "result = []\n",
    "for line in tqdm(data):\n",
    "    noun_phrase = line['key'].replace(\"_\", \" \")\n",
    "    property = line['attribute']\n",
    "    \n",
    "    token = merge_phrases(nlp(noun_phrase))[0]\n",
    "    root = token._.root\n",
    "\n",
    "    if root not in token._.C:\n",
    "        continue\n",
    "    modifier = [concept for concept in token._.C if concept != root]\n",
    "\n",
    "    if len(modifier) > 0:\n",
    "        modifier = modifier[0]\n",
    "\n",
    "        result.append(dict(\n",
    "            noun_phrase=noun_phrase,\n",
    "            root=root,\n",
    "            modifier=modifier,\n",
    "            property=property,\n",
    "            type=\"\"\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset/add.jsonl\", 'w') as outfile:\n",
    "    for entry in result:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conceptualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import json\n",
    "\n",
    "fpath = \"dataset/conceptual_combination.jsonl\"\n",
    "\n",
    "with jsonlines.open(fpath) as file:\n",
    "    data = list(file.iter())\n",
    "    \n",
    "fpath = \"dataset/processed/sample_0904.json\"\n",
    "with open(fpath) as f:\n",
    "    raw = json.load(f)\n",
    "    raw = {line[0].replace(\"\\n\", \" \"): line for line in raw}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def capitalize_sentences(text):\n",
    "    # Split the text into sentences using regex (looking for punctuation followed by a space)\n",
    "    sentences = re.split(r'(?<=[.!?]) +', text)\n",
    "    \n",
    "    # Capitalize each sentence\n",
    "    capitalized_sentences = [sentence.capitalize() for sentence in sentences]\n",
    "    \n",
    "    # Join the sentences back together with a space\n",
    "    return ' '.join(capitalized_sentences)\n",
    "\n",
    "def sent_noun_phrase_indexing(sent, noun_phrase):\n",
    "    sent = sent.lower()\n",
    "    noun_phrase = noun_phrase.lower()\n",
    "    index = sent.find(noun_phrase)\n",
    "    sent = sent[:index] + '[' + sent[index:index+len(noun_phrase)] + ']' + sent[index+len(noun_phrase):]\n",
    "    return capitalize_sentences(sent), noun_phrase.capitalize()\n",
    "\n",
    "template = \"\"\"Following the given examples, you are required to conceptualize the instance (enclosed by []) in the last given noun phrase into abstract concepts. The concept should still fit into the instance’s original sentence. Make sure that the generated abstract concepts are general and not simply hypernyms of the instance.\n",
    "---\n",
    "Noun phrase <1>: Julia looks nervous like a firefly in front of a chameleon. [A firefly in front of a chameleon] can be conceptualized as Prey Facing Predator.\n",
    "Noun phrase <2>: All up together like [a brown apple] when he is dried up, like this way! [A brown apple] can be conceptualized as Unusually Colored Object.\n",
    "Noun phrase <3>: You're not going to go storming in there like [a bull in a china shop again]. [A bull in a china shop] can be conceptualized as Unlikely Object in a Certain Place.\n",
    "Noun phrase <4>: Our economy will be as stable as [an apple on a toothpick]. [An apple on a toothpick] can be conceptualized as Object in an Unstable State.\n",
    "Noun phrase <5>: {sent}, [{noun_phrase}] can be conceptualized as\"\"\"\n",
    "\n",
    "prompts = []\n",
    "\n",
    "for line in data:\n",
    "    noun_phrase = line['noun_phrase']\n",
    "    sent = raw[noun_phrase][-1]\n",
    "\n",
    "    sent, noun_phrase = sent_noun_phrase_indexing(sent, noun_phrase)\n",
    "    prompts.append((sent, noun_phrase, template.format(sent=sent, noun_phrase=noun_phrase)))\n",
    "    \n",
    "prompts = [dict(\n",
    "    custom_id = \"request-{i}-{np}\".format(i=i+1, np=noun_phrase.replace(\" \", \"_\")),\n",
    "    method = \"POST\",\n",
    "    url = \"/v1/chat/completions\",\n",
    "    body = dict(\n",
    "        model = \"gpt-4o-mini\",\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens = 100,\n",
    "        n=10,\n",
    "        temperature=0.7,\n",
    "        top_p=0.8,\n",
    "    )\n",
    ") for i, (sent, noun_phrase, prompt) in enumerate(prompts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "with open(\"dataset/processed/conceptualization_chatgpt.jsonl\", 'w') as outfile:\n",
    "    for entry in prompts:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')\n",
    "        \n",
    "client = OpenAI(api_key=)\n",
    "\n",
    "batch_input_file = client.files.create(\n",
    "  file=open(\"dataset/processed/conceptualization_chatgpt.jsonl\", \"rb\"),\n",
    "  purpose=\"batch\"\n",
    ")\n",
    "\n",
    "batch_input_file_id = batch_input_file.id\n",
    "\n",
    "client.batches.create(\n",
    "    input_file_id=batch_input_file_id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\",\n",
    "    metadata={\n",
    "      \"description\": \"extract property of noun phrase from sentence\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import json\n",
    "\n",
    "model_output = dict()\n",
    "with jsonlines.open(\"dataset/processed/result/batch_WW0jFjwNYDuGwLkDzNkZ6g3x_output.jsonl\") as f:\n",
    "    for line in f.iter():\n",
    "        model_output[line['custom_id'].split(\"-\")[-1]] = [choice['message']['content'] for choice in line['response']['body']['choices']]\n",
    "    model_output = {key.lower().replace(\"_\", \" \"): model_output[key] for key in model_output}\n",
    "    \n",
    "for line in data:\n",
    "    line['conceptualization'] = model_output[line['noun_phrase'].lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset/conceptual_combination_conceptualization.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in data:\n",
    "        json.dump(line, f, ensure_ascii=False) # ensure_ascii로 한글이 깨지지 않게 저장\n",
    "        f.write(\"\\n\") # json을 쓰는 것과 같지만, 여러 줄을 써주는 것이므로 \"\\n\"을 붙여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "\n",
    "data = []\n",
    "with jsonlines.open(\"dataset/conceptual_combination_conceptualization.jsonl\") as f:\n",
    "    for line in f.iter():\n",
    "        data.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import jsonlines\n",
    "\n",
    "for line in tqdm(data):\n",
    "    prompt = \"{noun_phrase} is {category}\"\n",
    "\n",
    "    noun_phrase = line['noun_phrase'].capitalize()\n",
    "    categories = [cat.lower() for cat in line['conceptualization']]\n",
    "    \n",
    "    line['conceptualization'] = [categories[i] for i, score in enumerate(get_score([prompt.format(noun_phrase=noun_phrase, category=cat) for cat in categories])) if score > 0.9]\n",
    "\n",
    "data = []\n",
    "with jsonlines.open(\"dataset/conceptual_combination_conceptualization_filtered.jsonl\") as f:\n",
    "    for line in f.iter():\n",
    "        data.append(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonlines(fpath):\n",
    "    import jsonlines # !pip install jsonlines 해주기\n",
    "    data = []\n",
    "    with jsonlines.open(fpath) as read_file:\n",
    "        for line in read_file.iter():\n",
    "            data.append(line)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def capitalize_sentences(text):\n",
    "    # Split the text into sentences using regex (looking for punctuation followed by a space)\n",
    "    sentences = re.split(r'(?<=[.!?]) +', text)\n",
    "    \n",
    "    # Capitalize each sentence\n",
    "    capitalized_sentences = [sentence.capitalize() for sentence in sentences]\n",
    "    \n",
    "    # Join the sentences back together with a space\n",
    "    return ' '.join(capitalized_sentences)\n",
    "\n",
    "def sent_noun_phrase_indexing(sent, noun_phrase):\n",
    "    sent = sent.lower()\n",
    "    noun_phrase = noun_phrase.lower()\n",
    "    index = sent.find(noun_phrase)\n",
    "    sent = sent[:index] + '[' + sent[index:index+len(noun_phrase)] + ']' + sent[index+len(noun_phrase):]\n",
    "    return capitalize_sentences(sent), noun_phrase.capitalize()\n",
    "\n",
    "template = \"\"\"Following the given examples, you are required to conceptualize the instance (enclosed by []) in the last given noun phrase into abstract concepts. The concept should still fit into the instance’s original sentence. Make sure that the generated abstract concepts are general and not simply hypernyms of the instance.\n",
    "---\n",
    "Noun phrase <1>: Julia looks nervous like a firefly in front of a chameleon. [A firefly in front of a chameleon] can be conceptualized as Prey Facing Predator.\n",
    "Noun phrase <2>: All up together like [a brown apple] when he is dried up, like this way! [A brown apple] can be conceptualized as Unusually Colored Object.\n",
    "Noun phrase <3>: You're not going to go storming in there like [a bull in a china shop again]. [A bull in a china shop] can be conceptualized as Unlikely Object in a Certain Place.\n",
    "Noun phrase <4>: Our economy will be as stable as [an apple on a toothpick]. [An apple on a toothpick] can be conceptualized as Object in an Unstable State.\n",
    "Noun phrase <5>: {sent}, [{noun_phrase}] can be conceptualized as\"\"\"\n",
    "\n",
    "prompts = []\n",
    "\n",
    "data = read_jsonlines(\"dataset/conceptual_combination_conceptualization_filtered.jsonl\")\n",
    "data = [line for line in data if len(line['conceptualization']) > 0]\n",
    "\n",
    "for line in data:\n",
    "    noun_phrase = line['noun_phrase']\n",
    "    sent = raw[noun_phrase][-1]\n",
    "    categories = line['conceptualization']\n",
    "\n",
    "    sent, noun_phrase = sent_noun_phrase_indexing(sent, noun_phrase)\n",
    "    prompts.append((sent, noun_phrase, template.format(sent=sent, noun_phrase=noun_phrase)))\n",
    "    \n",
    "prompts = [dict(\n",
    "    custom_id = \"request-{i}-{np}\".format(i=i+1, np=noun_phrase.replace(\" \", \"_\")),\n",
    "    method = \"POST\",\n",
    "    url = \"/v1/chat/completions\",\n",
    "    body = dict(\n",
    "        model = \"gpt-4o-mini\",\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens = 100,\n",
    "        n=10,\n",
    "        temperature=0.7,\n",
    "        top_p=0.8,\n",
    "    )\n",
    ") for i, (sent, noun_phrase, prompt) in enumerate(prompts)]\n",
    "\n",
    "with open(\"dataset/processed/conceptualization_chatgpt.jsonl\", 'w') as outfile:\n",
    "    for entry in prompts:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "from transformers import pipeline\n",
    "\n",
    "# load tagger\n",
    "tagger = SequenceTagger.load(\"flair/pos-english\")\n",
    "\n",
    "def entity_tag(sentence_list) :\n",
    "    sentence = Sentence(sentence_list)\n",
    "    tagger.predict(sentence)\n",
    "    return sentence \n",
    "\n",
    "ner_tag_cnt = 0\n",
    "cnt = 0\n",
    "phrases = []\n",
    "ner_phrases = []\n",
    "not_ner_phrases = []\n",
    "\n",
    "with jsonlines.open('dataset/conceptual_combination_type_annotated.jsonl', \"r\") as fp:\n",
    "    for line in fp.iter(type=dict):\n",
    "        cnt += 1\n",
    "        phrases.append(line)\n",
    "        \n",
    "for _, line in enumerate(tqdm(phrases)) :\n",
    "    tags = [i.tag for i in entity_tag(line['noun_phrase'])]\n",
    "    if ('NNPS' in tags) or ('FW' in tags) or ('NNP' in tags) :\n",
    "        # print(line['noun_phrase'], tags)\n",
    "        ner_phrases.append(line)\n",
    "        ner_tag_cnt += 1\n",
    "    else:\n",
    "        not_ner_phrases.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(not_ner_phrases), len(ner_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset/processed/conceptualization_chatgpt_ner_filtered.jsonl\", 'w') as outfile:\n",
    "    for entry in not_ner_phrases:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import json\n",
    "\n",
    "fpath = \"dataset/processed/intermediate_vera.jsonl\"\n",
    "\n",
    "with jsonlines.open(fpath) as file:\n",
    "    data = list(file.iter())\n",
    "    \n",
    "fpath = \"dataset/processed/sample_0904.json\"\n",
    "with open(fpath) as f:\n",
    "    raw = json.load(f)\n",
    "    raw = {line[0].replace(\"\\n\", \" \"): line for line in raw}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in data:\n",
    "    line['sentence'] = raw[line['noun_phrase']][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.7\n",
    "\n",
    "def identify_property_type(line):\n",
    "    plausible_results = dict(\n",
    "        root = \"Yes\" in line['output_root'][0] if line['output_root'][1] > threshold else None,\n",
    "        modifier = \"Yes\" in line['output_modifier'][0] if line['output_modifier'][1] > threshold else None\n",
    "    )\n",
    "    \n",
    "    if True in plausible_results.values():\n",
    "        line['type'] = 'component'\n",
    "    elif list(plausible_results.values()).count(False) == 2:\n",
    "        line['type'] = 'phrase'\n",
    "    else:\n",
    "        line['type'] = None\n",
    "    line['sentence'] = line['sentence'].replace(\"\\n\", \" \")\n",
    "\n",
    "    return line\n",
    "\n",
    "def contains_digit(s):\n",
    "    return any(char.isdigit() for char in s)\n",
    "\n",
    "result = [identify_property_type(line) for line in data]\n",
    "result = [line for line in result if line['type'] == 'phrase']\n",
    "result = [line for line in result if not contains_digit(line['noun_phrase'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleantext.sklearn import CleanTransformer\n",
    "\n",
    "cleaner = CleanTransformer(no_punct=False, lower=True)\n",
    "\n",
    "for line in result:\n",
    "    line['noun_phrase'] = cleaner.transform([line['noun_phrase']])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter([line['type'] for line in result]).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_concepts = [\"lot\", \"black\", \"cock\", \"negro\", \"sex\", \"fking\", \"bunch\"]\n",
    "\n",
    "result = [line for line in result if not (line['root'] in pass_concepts or line['modifier'] in pass_concepts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset/processed/intermediate_vera_phrase.jsonl\", 'w') as outfile:\n",
    "    for entry in result:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = \"dataset/processed/intermediate_vera_phrase.jsonl\"\n",
    "\n",
    "with jsonlines.open(fpath) as file:\n",
    "    data = list(file.iter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [line for line in data if 'while' not in line['output_modifier'][0].lower() and 'while' not in line['output_root'][0].lower()]\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [line for line in data if len(list(set(line['noun_phrase'].lower().split()) & set(line['property'].lower().split()))) == 0]\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset/processed/intermediate_vera_phrase_whilefilter.jsonl\", 'w') as outfile:\n",
    "    for entry in data:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add canceled property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "\n",
    "# Extract phrase property and get canceled properties\n",
    "with jsonlines.open(\"dataset/processed/intermediate_vera_phrase_whilefilter_filtered.jsonl\") as f:\n",
    "    data = [line for line in f]\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.kb_model import MAPSKB\n",
    "\n",
    "kb = MAPSKB(\"src/model/MAPS-KB.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import transformers\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import openai\n",
    "import backoff\n",
    "import copy\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import backoff\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import base64\n",
    "import os, sys, pathlib, json, pdb\n",
    "import concurrent.futures\n",
    "\n",
    "class ParallelGPT():\n",
    "    def __init__(self, model_id):\n",
    "        self.model_id = model_id\n",
    "        self.client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
    "    @backoff.on_exception(backoff.expo, (openai.RateLimitError, openai.APIError, openai.Timeout, openai.BadRequestError, openai.APIConnectionError, openai.InternalServerError))\n",
    "    def completion_with_backoff(self, **kwargs):\n",
    "        return self.client.chat.completions.create(**kwargs)\n",
    "\n",
    "    def generate(self, text, image=None, max_new_tokens=2048, temperature=1, num_return_sequences=1, **kwargs):\n",
    "        if isinstance(text, str):\n",
    "            text = [text]\n",
    "        if image is not None:\n",
    "            if isinstance(image, str):\n",
    "                image = [image]\n",
    "            assert len(text) == len(image)\n",
    "\n",
    "            def process_text_and_image(t, i, idx):\n",
    "                base64_image = encode_image(i)\n",
    "                completion = self.completion_with_backoff(\n",
    "                    model=self.model_id,\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": [\n",
    "                                {\n",
    "                                    \"type\": \"text\", \"text\": t\n",
    "                                },\n",
    "                                {\n",
    "                                    \"type\": \"image_url\",\n",
    "                                    \"image_url\": \n",
    "                                    {\n",
    "                                        \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                                    },\n",
    "                                },\n",
    "                            ],\n",
    "                        }\n",
    "                    ],\n",
    "                    max_tokens=max_new_tokens,\n",
    "                    temperature=temperature,\n",
    "                    n=num_return_sequences,\n",
    "                    **kwargs\n",
    "                )\n",
    "                return (completion, idx)\n",
    "\n",
    "\n",
    "            with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "                futures = [executor.submit(process_text_and_image, t, i, idx) for idx, t, i in zip(range(len(text)), text, image)]\n",
    "                completions = []\n",
    "                for future in concurrent.futures.as_completed(futures):\n",
    "                    completions.append(future.result())\n",
    "\n",
    "            completions_sorted = sorted(completions, key=lambda x: x[1])\n",
    "            responses = [[completion[0].choices[i].message.content for i in range(num_return_sequences)] for completion in completions_sorted]\n",
    "            completions = [completion[0] for completion in completions_sorted]\n",
    "\n",
    "\n",
    "            return {'responses': responses, 'completions': completions}\n",
    "\n",
    "        else:\n",
    "\n",
    "            def process_text(t, idx):\n",
    "                completion = self.completion_with_backoff(\n",
    "                    model=self.model_id,\n",
    "                    messages=[\n",
    "                            {\"role\": \"user\",\"content\": t,}\n",
    "                        \n",
    "                    ],\n",
    "                    max_tokens=max_new_tokens,\n",
    "                    temperature=temperature,\n",
    "                    n=num_return_sequences,\n",
    "                    **kwargs\n",
    "                )\n",
    "                return (completion, idx)\n",
    "\n",
    "\n",
    "            with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "                futures = [executor.submit(process_text, t, idx) for idx, t in enumerate(text)]\n",
    "                completions = []\n",
    "                for future in concurrent.futures.as_completed(futures):\n",
    "                    completions.append(future.result())\n",
    "\n",
    "            completions_sorted = sorted(completions, key=lambda x: x[1])\n",
    "            responses = [[completion[0].choices[i].message.content for i in range(num_return_sequences)] for completion in completions_sorted]\n",
    "            completions = [completion[0] for completion in completions_sorted]\n",
    "\n",
    "\n",
    "            return {'responses': responses, 'completions': completions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_properties(line):\n",
    "    template = \"\"\"List the properties of given concept. Return the response in following template: **{{property_1}}**, **{{property_2}}**, **{{property_3}}**, ..\n",
    "\n",
    "---\n",
    "Concept: Apple\n",
    "Attribute: **Red**, **Round**\n",
    "\n",
    "Concept: Winter\n",
    "Attribute: **Cold**, **Snowy**, **Dry**\n",
    "\n",
    "Concept: {concept}\n",
    "Attribute: \"\"\"\n",
    "\n",
    "    return [template.format(concept=line['root']), template.format(concept=line['modifier'])]\n",
    "    \n",
    "def get_component_property(concept, attributes):\n",
    "    prompt = \"\\\"{concept}\\\" can be characterized by being/having \\\"{attribute}\\\".\"\n",
    "    if len(attributes) == 0:\n",
    "        return []\n",
    "    component_index = [i for i, score in enumerate(get_score([prompt.format(concept=concept, attribute=attribute) for attribute in attributes])) if score > 0.7]\n",
    "    return [attributes[index] for index in component_index]\n",
    "\n",
    "model_inputs = [prompt for line in data for prompt in get_properties(line)]\n",
    "model = ParallelGPT(model_id=\"gpt-4o-mini\")\n",
    "results = model.generate(model_inputs, num_return_sequences=5)['responses']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def pairwise(lst):\n",
    "    return [lst[i:i+2] for i in range(0, len(lst), 2)]\n",
    "\n",
    "results = pairwise(results)\n",
    "for line, result in zip(data, results):\n",
    "    line['root_component'] = list(set([att.lower() for r in result[0] for att in re.findall(r\"\\*\\*(.*?)\\*\\*\", r)]))\n",
    "    line['modifier_component'] = list(set([att.lower() for r in result[1] for att in re.findall(r\"\\*\\*(.*?)\\*\\*\", r)]))\n",
    "\n",
    "    \n",
    "with open(\"dataset/processed/intermediate_vera_phrase_whilefilter_filtered_component.jsonl\", 'w') as outfile:\n",
    "    for entry in data:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "\n",
    "# Extract phrase property and get canceled properties\n",
    "with jsonlines.open(\"dataset/processed/intermediate_vera_phrase_whilefilter_filtered_component.jsonl\") as f:\n",
    "    data = [line for line in f]\n",
    "\n",
    "def get_cancelled_property(noun_phrase, attributes):\n",
    "    prompt = \"\\\"{concept}\\\" can be characterized by being/having \\\"{attribute}\\\".\"\n",
    "    if len(attributes) == 0:\n",
    "        return []\n",
    "    component_index = [i for i, score in enumerate(get_score([prompt.format(concept=noun_phrase, attribute=attribute) for attribute in attributes])) if score < 0.2]\n",
    "    return [attributes[index] for index in component_index]\n",
    "\n",
    "for line in data:\n",
    "    line['root_canceled'] = get_cancelled_property(line['noun_phrase'], line['root_component'])\n",
    "    line['modifier_canceled'] = get_cancelled_property(line['noun_phrase'], line['modifier_component'])\n",
    "    \n",
    "with open(\"dataset/processed/intermediate_vera_phrase_whilefilter_filtered_component_canceled.jsonl\", 'w') as outfile:\n",
    "    for entry in data:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "\n",
    "# Extract phrase property and get canceled properties\n",
    "with jsonlines.open(\"dataset/conceptual_combination_type_annotated_add_component_filtered2.jsonl\") as f:\n",
    "    data = [line for line in f]\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"dataset/conceptual_combination_type_annotated_add_component_filtered2.jsonl\", 'w') as outfile:\n",
    "    for entry in data:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from ast import literal_eval\n",
    "\n",
    "df = load_dataset(\"conceptnet5/conceptnet5\", split=\"train\")\n",
    "df = df.to_pandas()\n",
    "df = df[df['lang'] == 'en']\n",
    "\n",
    "df['arg1'] = df['arg1'].apply(lambda x: x.split(\"/\")[-1])\n",
    "df['arg2'] = df['arg2'].apply(lambda x: x.split(\"/\")[-1])\n",
    "\n",
    "df['weight'] = df['extra_info'].apply(lambda x: literal_eval(x)['weight'])\n",
    "df = df[df['weight'] >= 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_edges(concept):\n",
    "    lookup_df = df[(df['arg1'] == concept) | (df['arg2'] == concept)]\n",
    "    return lookup_df\n",
    "\n",
    "def format_series_to_triplet(df):\n",
    "    return list(set(df.apply(lambda x: (x['arg1'], x['rel'], x['arg2']), axis=1).to_list()))\n",
    "\n",
    "def get_common_edge(row):\n",
    "    concept1 = row['root']\n",
    "    concept2 = row['modifier']\n",
    "    \n",
    "    lookup_df_1 = get_all_edges(concept1)\n",
    "    lookup_df_2 = get_all_edges(concept2)\n",
    "    \n",
    "    ## first-hop relation\n",
    "    fh_lookup_df_1 = lookup_df_1[(lookup_df_1['arg1'] == concept2) | (lookup_df_1['arg2'] == concept2)]\n",
    "    if fh_lookup_df_1.shape[0] > 0:\n",
    "        return {f\"{concept1}_{concept2}\": (format_series_to_triplet(fh_lookup_df_1), None)}\n",
    "    \n",
    "    ## second-hop relation\n",
    "    adjacent_1 = set(lookup_df_1[['arg1', 'arg2']].to_numpy().flatten())\n",
    "    adjacent_2 = set(lookup_df_2[['arg1', 'arg2']].to_numpy().flatten())\n",
    "    \n",
    "    common_element = adjacent_1 & adjacent_2\n",
    "    if len(common_element) > 0:\n",
    "        adjacent_1 = lookup_df_1[(lookup_df_1['arg1'].apply(lambda x: x in common_element)) | (lookup_df_1['arg2'].apply(lambda x: x in common_element))]\n",
    "        adjacent_2 = lookup_df_2[(lookup_df_2['arg1'].apply(lambda x: x in common_element)) | (lookup_df_2['arg2'].apply(lambda x: x in common_element))]\n",
    "        return {f\"{concept1}_{concept2}\": (format_series_to_triplet(adjacent_1) + format_series_to_triplet(adjacent_2), common_element)}\n",
    "    else:    \n",
    "        return {f\"{concept1}_{concept2}\": ([], None)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Google-Ngram-Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "    \n",
    "df = pd.read_csv(\"dataset/google1M5G.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"conceptual_combination\")\n",
    "from dataset.processed.get_nounphrase import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    x = x.split(\" \")\n",
    "    x = \" \".join([s for s in x if re.search(r'[^a-zA-Z0-9\\s]', s)])\n",
    "    return x\n",
    "\n",
    "with multiprocessing.Pool(20) as p:\n",
    "    df['0'] = list(tqdm(p.imap(f, df['0']), total=df.shape[0]))\n",
    "    p.close()\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "\n",
    "def flatten(data):\n",
    "    return [sample for row in data for sample in row]\n",
    "\n",
    "def f(x):\n",
    "    return flatten([token._.C for token in nlp(x)])\n",
    "    \n",
    "with multiprocessing.Pool(20) as p:\n",
    "    df['concepts'] = list(tqdm(p.imap(f, df['0']), total=df.shape[0]))\n",
    "    p.close()\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    if 'soccer' in i:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np = \"American footballs\"\n",
    "concepts = merge_phrases(nlp(np))[0]._.C\n",
    "print(concepts)\n",
    "\n",
    "df[df['concepts'].apply(lambda x: set(concepts).issubset(set(x)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = \"0 1 2 3 4 5 6 7 8 9 _ADJ_ _ADP_ _ADV_ _CONJ_ _DET_ _NOUN_ _NUM_ _PRON_ _PRT_ _VERB_ a_ aa ab ac ad ae af ag ah ai aj ak al am an ao ap aq ar as at au av aw ax ay az b_ ba bb bc bd be bf bg bh bi bj bk bl bm bn bo bp bq br bs bt bu bv bw bx by bz c_ ca cb cc cd ce cf cg ch ci cj ck cl cm cn co cp cq cr cs ct cu cv cw cx cy cz d_ da db dc dd de df dg dh di dj dk dl dm dn do dp dq dr ds dt du dv dw dx dy dz e_ ea eb ec ed ee ef eg eh ei ej ek el em en eo ep eq er es et eu ev ew ex ey ez f_ fa fb fc fd fe ff fg fh fi fj fk fl fm fn fo fp fq fr fs ft fu fv fw fx fy fz g_ ga gb gc gd ge gf gg gh gi gj gk gl gm gn go gp gq gr gs gt gu gv gw gx gy gz h_ ha hb hc hd he hf hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz i_ ia ib ic id ie if ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz j_ ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz k_ ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz l_ la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz m_ ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz n_ na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz o_ oa ob oc od oe of og oh oi oj ok ol om on oo op oq or os ot other ou ov ow ox oy oz p_ pa pb pc pd pe pf pg ph pi pj pk pl pm pn po pp pq pr ps pt pu punctuation pv pw px py pz q_ qa qb qc qd qe qf qg qh qi qj ql qm qn qo qp qq qr qs qt qu qv qw qx qy qz r_ ra rb rc rd re rf rg rh ri rj rk rl rm rn ro rp rq rr rs rt ru rv rw rx ry rz s_ sa sb sc sd se sf sg sh si sj sk sl sm sn so sp sq sr ss st su sv sw sx sy sz t_ ta tb tc td te tf tg th ti tj tk tl tm tn to tp tq tr ts tt tu tv tw tx ty tz u_ ua ub uc ud ue uf ug uh ui uj uk ul um un uo up uq ur us ut uu uv uw ux uy uz v_ va vb vc vd ve vf vg vh vi vj vk vl vm vn vo vp vq vr vs vt vu vv vw vx vy vz w_ wa wb wc wd we wf wg wh wi wj wk wl wm wn wo wp wq wr ws wt wu wv ww wx wy wz x_ xa xb xc xd xe xf xg xh xi xj xk xl xm xn xo xp xq xr xs xt xu xv xw xx xy xz y_ ya yb yc yd ye yf yg yh yi yj yk yl ym yn yo yp yq yr ys yt yu yv yw yx yy yz z_ za zb zc zd ze zf zg zh zi zj zk zl zm zn zo zp zq zr zs zt zu zv zw zx zy zz\"\n",
    "\n",
    "# data = data.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google_ngram_downloader import readline_google_store\n",
    "# from tqdm import tqdm\n",
    "# import multiprocessing\n",
    "\n",
    "# urls = [f\"http://storage.googleapis.com/books/ngrams/books/googlebooks-eng-all-5gram-20120701-{d}.gz\" for d in data]\n",
    "# fnames = [f\"googlebooks-eng-all-5gram-20120701-{d}.gz\" for d in data]\n",
    "# data = list(zip(urls, fnames))\n",
    "\n",
    "# with multiprocessing.Pool(10) as p:\n",
    "#     data = list(tqdm(p.imap(get_google_ngram_corpus, data), total=len(data)))\n",
    "#     p.close()\n",
    "#     p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cc_sw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
